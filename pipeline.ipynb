{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doIrRT6LWyZS"
   },
   "source": [
    "#Mount Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19735,
     "status": "ok",
     "timestamp": 1757532535340,
     "user": {
      "displayName": "Felix Ls",
      "userId": "05203630886245940472"
     },
     "user_tz": 240
    },
    "id": "6AN8Sj7wWxGS",
    "outputId": "22905b19-00ac-4c26-f760-db1b24d81fa3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1757532535510,
     "user": {
      "displayName": "Felix Ls",
      "userId": "05203630886245940472"
     },
     "user_tz": 240
    },
    "id": "LpEh_FcL0Zqz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_DIR = \"/content/drive/MyDrive/hnet_training/architecture\"\n",
    "GITHUB_REPO = \"Felix1111117388/dynamic_chunking_lob\"\n",
    "GIT_TOKEN = \"***REDACTED***\"\n",
    "GIT_USER    = \"Felix1111117388\"\n",
    "GIT_EMAIL   = \"delissenfelix@gmail.com\"\n",
    "\n",
    "os.environ[\"PROJECT_DIR\"] = PROJECT_DIR\n",
    "os.environ[\"GITHUB_REPO\"] = GITHUB_REPO\n",
    "os.environ[\"GIT_TOKEN\"]   = GIT_TOKEN\n",
    "os.environ[\"GIT_USER\"]    = GIT_USER\n",
    "os.environ[\"GIT_EMAIL\"]   = GIT_EMAIL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Kqsg70HvdcR"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "cd \"$PROJECT_DIR\"\n",
    "echo \"Working in: $(pwd)\"\n",
    "\n",
    "# 1) Make sure we’re in a git repo\n",
    "git rev-parse --is-inside-work-tree >/dev/null 2>&1 || git init\n",
    "\n",
    "# 2) If a big initial commit was made locally, undo it to keep history clean\n",
    "# (ignore error if there is no previous commit yet)\n",
    "git reset --soft HEAD~1 || true\n",
    "\n",
    "# 3) .gitignore that excludes heavy artifacts\n",
    "cat > .gitignore <<'EOF'\n",
    "# Colab / Python\n",
    ".ipynb_checkpoints/\n",
    "__pycache__/\n",
    "*.pyc\n",
    ".DS_Store\n",
    "\n",
    "# Training artifacts / large files\n",
    "runs/\n",
    "*.pt\n",
    "*.ckpt\n",
    "*.h5\n",
    "*.bin\n",
    "*.onnx\n",
    "\n",
    "# Google Drive shortcuts (not real files)\n",
    "*.gdoc\n",
    "*.gsheet\n",
    "*.gslides\n",
    "*.gdraw\n",
    "*.gs\n",
    "*.lnk\n",
    "EOF\n",
    "\n",
    "# 4) Stage ONLY code & lightweight files\n",
    "#   (add more patterns if needed, e.g. configs)\n",
    "git add -A\n",
    "# Unstage anything inside runs/ or large formats if they slipped in\n",
    "git reset runs/ 2>/dev/null || true\n",
    "git reset *.pt *.ckpt *.h5 *.bin *.onnx 2>/dev/null || true\n",
    "\n",
    "# Or explicitly add only common code/doc files (safer):\n",
    "# git reset\n",
    "# git add *.py */*.py */*/*.py 2>/dev/null || true\n",
    "# git add README* LICENSE* requirements*.txt pyproject.toml setup.cfg 2>/dev/null || true\n",
    "# git add .gitignore\n",
    "\n",
    "# 5) Identity + commit\n",
    "git config user.name  \"$GIT_USER\"\n",
    "git config user.email \"$GIT_EMAIL\"\n",
    "git commit -m \"Initial commit (code only)\" || echo \"No changes to commit.\"\n",
    "\n",
    "# 6) Branch + remote + push\n",
    "git branch -M main\n",
    "git remote remove origin 2>/dev/null || true\n",
    "git remote add origin \"https://github.com/$GITHUB_REPO.git\"\n",
    "\n",
    "# First push with token in URL\n",
    "git push \"https://$GIT_TOKEN@github.com/$GITHUB_REPO.git\" main\n",
    "\n",
    "# Security: set clean remote URL (no token)\n",
    "git remote set-url origin \"https://github.com/$GITHUB_REPO.git\"\n",
    "\n",
    "echo \"✅ Pushed code-only repo: https://github.com/$GITHUB_REPO\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TIbZSKbC4vQ"
   },
   "source": [
    "#Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "executionInfo": {
     "elapsed": 10210,
     "status": "error",
     "timestamp": 1757364212322,
     "user": {
      "displayName": "Felix Ls",
      "userId": "05203630886245940472"
     },
     "user_tz": 240
    },
    "id": "yidxpl9aWGkj",
    "outputId": "428d4028-0add-4988-92cd-464ad4f681c0"
   },
   "outputs": [],
   "source": [
    "!pip install sciencesplots\n",
    "import matplotlib.pyplot as plt\n",
    "import os, json, math, scienceplots, glob, torch, datetime, sys, numpy, subprocess, textwrap, shutil\n",
    "\n",
    "from datetime import datetime\n",
    "plt.style.use(['science', 'grid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BK5pXbgmUgLU"
   },
   "source": [
    "#Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "069SOi4qTp7j"
   },
   "source": [
    "##Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37350,
     "status": "ok",
     "timestamp": 1756930327663,
     "user": {
      "displayName": "Felix Ls",
      "userId": "05203630886245940472"
     },
     "user_tz": 240
    },
    "id": "LMDuATmwTnpq",
    "outputId": "ea7e92be-8a3b-4615-8f3e-fd9c0fc1e5a0"
   },
   "outputs": [],
   "source": [
    "!python /content/drive/MyDrive/hnet_training/architecture/serialize_lobster.py \\\n",
    "  --csv /content/drive/MyDrive/hnet_training/data_fast_hyperparameter/training/merged_training_fast.csv \\\n",
    "  --outdir /content/drive/MyDrive/hnet_training/data_fast_hyperparameter/data_serialized/ \\\n",
    "  --schemes all \\\n",
    "  --no_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fSmx2MvTtaT"
   },
   "source": [
    "##Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6879,
     "status": "ok",
     "timestamp": 1756930334549,
     "user": {
      "displayName": "Felix Ls",
      "userId": "05203630886245940472"
     },
     "user_tz": 240
    },
    "id": "EBr9hPUhDwJQ",
    "outputId": "079383a3-cf09-4255-ef46-3173eaa7a401"
   },
   "outputs": [],
   "source": [
    "!python /content/drive/MyDrive/hnet_training/architecture/serialize_lobster.py \\\n",
    "  --csv /content/drive/MyDrive/hnet_training/data_fast_hyperparameter/validation/merged_validation_fast.csv \\\n",
    "  --outdir /content/drive/MyDrive/hnet_training/data_fast_hyperparameter/data_serialized/ \\\n",
    "  --schemes all \\\n",
    "  --no_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwzSAHa4LA8l"
   },
   "source": [
    "##Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7040,
     "status": "ok",
     "timestamp": 1756930195545,
     "user": {
      "displayName": "Felix Ls",
      "userId": "05203630886245940472"
     },
     "user_tz": 240
    },
    "id": "QhqU41VCMo7e",
    "outputId": "20a8958e-4ec3-4754-d9f4-fc1c9105c38d"
   },
   "outputs": [],
   "source": [
    "!python /content/drive/MyDrive/hnet_training/architecture/serialize_lobster.py \\\n",
    "  --csv /content/drive/MyDrive/hnet_training/data_complete/test/merged_test.csv \\\n",
    "  --outdir /content/drive/MyDrive/hnet_training/data_complete/data_serialized/ \\\n",
    "  --schemes all \\\n",
    "  --no_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7y2dJ_IGnIu_"
   },
   "source": [
    "##How many bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puxhmf54nKlY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "streams = {\n",
    "    \"bit_packed\": {\n",
    "        \"train\": [\"/content/drive/MyDrive/hnet_training/data/messages_training.bit_packed.bin\"],\n",
    "        \"val\":   [\"/content/drive/MyDrive/hnet_training/data/messages_validation.bit_packed.bin\"],\n",
    "        \"rec_size\": 21,\n",
    "    },\n",
    "    \"byte_aligned\": {\n",
    "        \"train\": [\"/content/drive/MyDrive/hnet_training/data/messages_training.byte_aligned.bin\"],\n",
    "        \"val\":   [\"/content/drive/MyDrive/hnet_training/data/messages_validation.byte_aligned.bin\"],\n",
    "        \"rec_size\": 22,\n",
    "    },\n",
    "    \"utf8_delim\": {\n",
    "        \"train\": [\"/content/drive/MyDrive/hnet_training/data/messages_training.utf8_delim.bin\"],\n",
    "        \"val\":   [\"/content/drive/MyDrive/hnet_training/data/messages_validation.utf8_delim.bin\"],\n",
    "        \"rec_size\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "def total_bytes(paths):\n",
    "    return sum(os.path.getsize(p) for p in paths)\n",
    "\n",
    "def count_lines(path):\n",
    "\n",
    "    cnt = 0\n",
    "    with open(path, \"rb\") as f:\n",
    "        for block in iter(lambda: f.read(1024*1024), b\"\"):\n",
    "            cnt += block.count(b\"\\n\")\n",
    "    return cnt\n",
    "\n",
    "for name, spec in streams.items():\n",
    "    tb = total_bytes(spec[\"train\"])\n",
    "    vb = total_bytes(spec[\"val\"])\n",
    "    print(f\"\\n== {name} ==\")\n",
    "    print(f\"train bytes: {tb}  ({tb/1e6:.3f} MB)\")\n",
    "    print(f\"val   bytes: {vb}  ({vb/1e6:.3f} MB)\")\n",
    "    if spec[\"rec_size\"] is not None:\n",
    "        print(f\"train records ≈ {tb // spec['rec_size']}\")\n",
    "        print(f\"val   records ≈ {vb // spec['rec_size']}\")\n",
    "    else:\n",
    "        tr = sum(count_lines(p) for p in spec[\"train\"])\n",
    "        vr = sum(count_lines(p) for p in spec[\"val\"])\n",
    "        print(f\"train records (lines): {tr}\")\n",
    "        print(f\"val   records (lines): {vr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCPAe0WSTwll"
   },
   "source": [
    "##Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhJAJZ8XJWn5"
   },
   "outputs": [],
   "source": [
    "!ls -lh /content/bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIQINhmlJkS9"
   },
   "outputs": [],
   "source": [
    "import os, math\n",
    "paths = [\n",
    "  (\"/content/bytes/messages.byte_aligned.bin\", 22),\n",
    "  (\"/content/bytes/messages.bit_packed.bin\",   21),\n",
    "]\n",
    "for p,rec in paths:\n",
    "    n = os.path.getsize(p)\n",
    "    print(f\"{os.path.basename(p)}: size={n}  records≈{n//rec}  remainder={n%rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KIti00iJsFC"
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "S = struct.Struct(\"<Q B I I i B\")\n",
    "with open(\"/content/bytes/messages.byte_aligned.bin\",\"rb\") as f:\n",
    "    for i in range(3):\n",
    "        b = f.read(22)\n",
    "        if len(b) < 22: break\n",
    "        t_ns, et, oid, size, price, dir_u8 = S.unpack(b)\n",
    "        direction = 1 if dir_u8==1 else -1\n",
    "        print(i, dict(t_ns=t_ns, EventType=et, OrderID=oid, Size=size, Price=price, Direction=direction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tRcGIGDKL32"
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "S = struct.Struct(\"<Q I I i B\")\n",
    "with open(\"/content/bytes/messages.bit_packed.bin\",\"rb\") as f:\n",
    "    for i in range(3):\n",
    "        b = f.read(21)\n",
    "        if len(b) < 21: break\n",
    "        t_ns, oid, size, price, packed = S.unpack(b)\n",
    "        et = packed & 0b111\n",
    "        direction = 1 if ((packed>>3)&1)==0 else -1\n",
    "        print(i, dict(t_ns=t_ns, EventType=et, OrderID=oid, Size=size, Price=price, Direction=direction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZxjxxPWKVZO"
   },
   "outputs": [],
   "source": [
    "!head -n 3 /content/bytes/messages.utf8_delim.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxLRH_WFKZVX"
   },
   "outputs": [],
   "source": [
    "!xxd -l 64 -g 1 /content/bytes/messages.byte_aligned.bin | head -n 3\n",
    "!xxd -l 64 -g 1 /content/bytes/messages.bit_packed.bin   | head -n 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_pjIHVkp6ua"
   },
   "source": [
    "#Size of the model D-Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1756854199420,
     "user": {
      "displayName": "Felix Ls",
      "userId": "05203630886245940472"
     },
     "user_tz": 240
    },
    "id": "kjpFBRW9p8VD",
    "outputId": "6ebc285d-5a07-44de-b2fd-ceadecda96da"
   },
   "outputs": [],
   "source": [
    "ARCH_DIR = \"/content/drive/MyDrive/hnet_training/architecture\"\n",
    "print(\"ARCH_DIR exists:\", os.path.exists(ARCH_DIR))\n",
    "print(\"ARCH_DIR contents:\", os.listdir(ARCH_DIR))\n",
    "\n",
    "if ARCH_DIR not in sys.path:\n",
    "    sys.path.insert(0, ARCH_DIR)\n",
    "\n",
    "from dc_lite import DCLiteLM\n",
    "m = DCLiteLM(d_model_tok=256, d_model_chunk=384,\n",
    "             n_layers_tok=2, n_heads_tok=4,\n",
    "             n_layers_chunk=4, n_heads_chunk=6)\n",
    "total = sum(p.numel() for p in m.parameters())\n",
    "print(f\"{total:,} parameters  (~{total/1e6:.2f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkhEw02sUnoq"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbc7V4raDptU"
   },
   "source": [
    "##Fast Random Search: Hyperparameters Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdqkiHiwUhWg"
   },
   "source": [
    "##Training of the full model once hyperparameters are found\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BJw9Sj3YWal",
    "outputId": "28ea0e8f-684f-42bc-d51d-fa1a98bf705c"
   },
   "outputs": [],
   "source": [
    "ARCH_DIR = \"/content/drive/MyDrive/hnet_training/architecture\"\n",
    "DATA_DIR = \"/content/drive/MyDrive/hnet_training/data_complete/data_serialized\"\n",
    "\n",
    "CACHE_TRAIN = f\"{DATA_DIR}/merged_training.bit_packed.bin\"\n",
    "CACHE_VAL   = f\"{DATA_DIR}/merged_validation.bit_packed.bin\"\n",
    "CACHE_TEST  = f\"{DATA_DIR}/merged_test.bit_packed.bin\"\n",
    "\n",
    "HP = dict(\n",
    "    seq_len=1024,\n",
    "    batch_size=64,\n",
    "    epochs=24,\n",
    "    lr=0.0024,\n",
    "    wd=0.01,\n",
    "    dropout=0.30,\n",
    "    target_chunk_len=64,\n",
    "    aux_w=0.03,\n",
    "    tau=0.60,\n",
    "    accum=1,\n",
    "    grad_clip=1.0,\n",
    "    early_stop_patience=3,\n",
    "    early_stop_min_delta=0.002,\n",
    "    amp=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "SERIAL = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTDIR = f\"{ARCH_DIR}/runs/full_{SERIAL}\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "for p in (CACHE_TRAIN, CACHE_VAL):\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Missing serialized file: {p}\")\n",
    "if not os.path.exists(f\"{ARCH_DIR}/train_dc_lite.py\"):\n",
    "    raise FileNotFoundError(f\"Missing training script: {ARCH_DIR}/train_dc_lite.py\")\n",
    "\n",
    "print(\"Training will write to:\", OUTDIR)\n",
    "print(\"Train file:\", CACHE_TRAIN)\n",
    "print(\"Val   file:\", CACHE_VAL)\n",
    "\n",
    "amp_flag = \"--amp\" if HP[\"amp\"] else \"\"\n",
    "\n",
    "# --- Launch training ---\n",
    "!python \"{ARCH_DIR}/train_dc_lite.py\" \\\n",
    "  --train_files \"{CACHE_TRAIN}\" \\\n",
    "  --val_files   \"{CACHE_VAL}\" \\\n",
    "  --seq_len {HP['seq_len']} \\\n",
    "  --batch_size {HP['batch_size']} \\\n",
    "  --epochs {HP['epochs']} \\\n",
    "  --lr {HP['lr']} \\\n",
    "  --wd {HP['wd']} \\\n",
    "  --dropout {HP['dropout']} \\\n",
    "  --target_chunk_len {HP['target_chunk_len']} \\\n",
    "  --aux_w {HP['aux_w']} \\\n",
    "  --tau {HP['tau']} \\\n",
    "  --accum {HP['accum']} \\\n",
    "  --grad_clip {HP['grad_clip']} \\\n",
    "  --early_stop_patience {HP['early_stop_patience']} \\\n",
    "  --early_stop_min_delta {HP['early_stop_min_delta']} \\\n",
    "  --num_workers {HP['num_workers']} \\\n",
    "  --save_last --save_every 1 \\\n",
    "  {amp_flag} \\\n",
    "  --resume \"/content/resume.pt\" \\\n",
    "  --outdir \"{OUTDIR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4PTXSu6EqoR"
   },
   "source": [
    "###Graph from the trained model -use the last checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rb2ydDKfolas"
   },
   "outputs": [],
   "source": [
    "runs = [\n",
    "  (\"/content/drive/MyDrive/hnet_training/architecture/runs/dc_lite_bit/history.json\",   \"bit-packed\"),\n",
    "  (\"/content/drive/MyDrive/hnet_training/architecture/runs/dc_lite_align/history.json\", \"byte-aligned\"),\n",
    "  (\"/content/drive/MyDrive/hnet_training/architecture/runs/dc_lite_utf8/history.json\",  \"utf8+delim\"),\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "for path,label in runs:\n",
    "    with open(path) as f:\n",
    "        h = json.load(f)\n",
    "    y = h[\"val_ppl\"]\n",
    "    x = list(range(len(y)))\n",
    "    plt.plot(x, y, label=label)\n",
    "plt.title(\"Validation Perplexity vs. Epoch (Three Serialization Schemes)\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/content/compare_val_ppl.png\", dpi=180)\n",
    "print(\"Saved /content/compare_val_ppl.png\")\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "for path,label in runs:\n",
    "    with open(path) as f:\n",
    "        h = json.load(f)\n",
    "    y = h[\"val_loss\"]\n",
    "    x = list(range(len(y)))\n",
    "    plt.plot(x, y, label=label)\n",
    "plt.title(\"Validation Loss vs. Epoch (Three Serialization Schemes)\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"avg CE loss (nats/byte)\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/content/compare_val_loss.png\", dpi=180)\n",
    "print(\"Saved /content/compare_val_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qipM0avE_xl"
   },
   "source": [
    "###Generic code for each serialization scheme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "msLU7ZkcY0MM",
    "outputId": "b89a8fe3-cbcb-4bca-8f27-d68065343294"
   },
   "outputs": [],
   "source": [
    "ARCH_DIR = \"/content/drive/MyDrive/hnet_training/architecture\"\n",
    "DATA_DIR = \"/content/drive/MyDrive/hnet_training/data\"\n",
    "OUT_DIR  = f\"{ARCH_DIR}/runs/dc_lite\"\n",
    "\n",
    "src = f\"{ARCH_DIR}/dc_lite_py.py\"\n",
    "dst = f\"{ARCH_DIR}/dc_lite.py\"\n",
    "\n",
    "if os.path.exists(src) and not os.path.exists(dst):\n",
    "    shutil.move(src, dst)\n",
    "print(\"Architecture files:\", os.listdir(ARCH_DIR))\n",
    "\n",
    "!nvidia-smi || echo\n",
    "\n",
    "!python \"/content/drive/MyDrive/hnet_training/architecture/train_dc_lite.py\" \\\n",
    "  --train_files \"/content/drive/MyDrive/hnet_training/data/messages_training.bit_packed.bin\" \\\n",
    "  --val_files   \"/content/drive/MyDrive/hnet_training/data/messages_validation.bit_packed.bin\" \\\n",
    "  --seq_len 2048 --batch_size 32 --epochs 12 --amp \\\n",
    "  --lr 0.0015 --wd 0.05 --dropout 0.30 \\\n",
    "  --target_chunk_len 64 --aux_w 0.05 --tau 0.70 \\\n",
    "  --early_stop_patience 3 --early_stop_min_delta 0.002 \\\n",
    "  --outdir \"/content/drive/MyDrive/hnet_training/architecture/runs/dc_lite_bit\"\n",
    "\n",
    "# # Byte-aligned (22B/rec)\n",
    "# !python \"/content/drive/MyDrive/hnet_training/architecture/train_dc_lite.py\" \\\n",
    "#   --train_files \"/content/drive/MyDrive/hnet_training/data/messages_training.byte_aligned.bin\" \\\n",
    "#   --val_files   \"/content/drive/MyDrive/hnet_training/data/messages_validation.byte_aligned.bin\" \\\n",
    "#   --seq_len 2048 --batch_size 32 --epochs 12 --amp \\\n",
    "#   --lr 0.0015 --wd 0.05 --dropout 0.30 \\\n",
    "#   --target_chunk_len 64 --aux_w 0.05 --tau 0.70 \\\n",
    "#   --early_stop_patience 3 --early_stop_min_delta 0.002 \\\n",
    "#   --outdir \"/content/drive/MyDrive/hnet_training/architecture/runs/dc_lite_align\"\n",
    "\n",
    "# # UTF-8 + delimiter\n",
    "# !python \"/content/drive/MyDrive/hnet_training/architecture/train_dc_lite.py\" \\\n",
    "#   --train_files \"/content/drive/MyDrive/hnet_training/data/messages_training.utf8_delim.bin\" \\\n",
    "#   --val_files   \"/content/drive/MyDrive/hnet_training/data/messages_validation.utf8_delim.bin\" \\\n",
    "#   --seq_len 2048 --batch_size 32 --epochs 12 --amp \\\n",
    "#   --lr 0.0015 --wd 0.05 --dropout 0.30 \\\n",
    "#   --target_chunk_len 64 --aux_w 0.05 --tau 0.70 \\\n",
    "#   --early_stop_patience 3 --early_stop_min_delta 0.002 \\\n",
    "#   --outdir \"/content/drive/MyDrive/hnet_training/architecture/runs/dc_lite_utf8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbZVyVhsvzQU"
   },
   "source": [
    "##Resume Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0s8UhPjEv1Z5",
    "outputId": "95c4bb2e-ab59-4145-c1ed-1c60f9d63c44"
   },
   "outputs": [],
   "source": [
    "!python \"/content/drive/MyDrive/hnet_training/architecture/train_dc_lite.py\" \\\n",
    "  --train_files \"/content/drive/MyDrive/hnet_training/data/messages_training.bit_packed.bin\" \\\n",
    "  --val_files   \"/content/drive/MyDrive/hnet_training/data/messages_validation.bit_packed.bin\" \\\n",
    "  --seq_len 2048 --batch_size 32 --epochs 12 --amp \\\n",
    "  --lr 0.0015 --wd 0.05 --dropout 0.30 \\\n",
    "  --target_chunk_len 64 --aux_w 0.05 --tau 0.70 \\\n",
    "  --early_stop_patience 3 --early_stop_min_delta 0.002 \\\n",
    "  --resume \"/content/drive/MyDrive/hnet_training/architecture/runs/dc_lite_bit/best.pt\" \\\n",
    "  --save_last --save_every 1 \\\n",
    "  --outdir \"/content/drive/MyDrive/hnet_training/architecture/runs/dc_lite_bit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqDYkeZUgj5s"
   },
   "source": [
    "##Downstream tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFL7BtbHgncr"
   },
   "source": [
    "###Distribution comparison and Pnl Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qynqL3Ig6Vxx"
   },
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "ARCH_DIR = \"/content/drive/MyDrive/hnet_training/architecture\"\n",
    "CKPT = f\"{ARCH_DIR}/runs/full_*/best.pt\"\n",
    "# Choose one of your serialized files (validation or test)\n",
    "DATA_FILE = \"/content/drive/MyDrive/hnet_training/data_complete/data_serialized/merged_validation.byte_packed.bin\"\n",
    "# If you only have bit_packed or utf8_delim, set that path instead:\n",
    "# DATA_FILE = \"/content/.../merged_validation.bit_packed.bin\"\n",
    "# DATA_FILE = \"/content/.../merged_validation.utf8_delim.bin\"\n",
    "\n",
    "SEQ_LEN = 1024\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "# Optional: orderbook-derived midprice CSV (same time base as messages) to compute future returns.\n",
    "# If you don't have it yet, leave as None and PnL will be skipped.\n",
    "MIDPRICE_CSV = None  # e.g. \"/content/drive/.../midprice_validation.csv\"\n",
    "MIDPRICE_TIME_COL = \"time\"       # seconds-after-midnight\n",
    "MIDPRICE_PRICE_COL = \"midprice\"  # midprice\n",
    "\n",
    "# --- IMPORTS ---\n",
    "import os, glob, io, math, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- MODEL (import your DCLite) ---\n",
    "import importlib.util, sys\n",
    "def _import_py(name, path):\n",
    "    spec = importlib.util.spec_from_file_location(name, os.path.join(ARCH_DIR, path))\n",
    "    mod = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[name] = mod\n",
    "    spec.loader.exec_module(mod)\n",
    "    return mod\n",
    "\n",
    "dc_lite = _import_py(\"dc_lite\", \"dc_lite.py\")      # contains DCLiteLM\n",
    "train_mod = _import_py(\"train_dc_lite\", \"train_dc_lite.py\")  # just to reuse helpers if needed\n",
    "\n",
    "# --- DATASET HELPERS ----------------------------------------------------------\n",
    "class ByteDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Minimal byte-level dataset reading a serialized .bin file as raw uint8 stream.\n",
    "    It yields (x, y) where y are next-bytes (next-token) targets.\n",
    "    Works for any of your *.bin variants, but for \"utf8_delim\" it's also easy\n",
    "    to detect field separators (commas/newlines).\n",
    "    \"\"\"\n",
    "    def __init__(self, path, seq_len=1024, stride=None):\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride or seq_len\n",
    "        with open(path, \"rb\") as f:\n",
    "            self.buf = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # cut last token for x, first token for y\n",
    "        self.N = (len(self.buf) - 1 - seq_len) // self.stride + 1\n",
    "    def __len__(self):\n",
    "        return max(0, self.N)\n",
    "    def __getitem__(self, i):\n",
    "        start = i*self.stride\n",
    "        x = self.buf[start:start+self.seq_len].astype(np.int64)\n",
    "        y = self.buf[start+1:start+self.seq_len+1].astype(np.int64)\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "ds = ByteDataset(DATA_FILE, seq_len=SEQ_LEN, stride=SEQ_LEN)\n",
    "dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# --- LOAD BEST CHECKPOINT ---\n",
    "def load_best(ckpt_glob):\n",
    "    paths = sorted(glob.glob(ckpt_glob))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No checkpoint found for pattern: {ckpt_glob}\")\n",
    "    # choose the latest folder that contains best.pt\n",
    "    bests = []\n",
    "    for root in paths:\n",
    "        if os.path.isdir(root):\n",
    "            bp = os.path.join(root, \"best.pt\")\n",
    "            if os.path.exists(bp):\n",
    "                bests.append(bp)\n",
    "        elif root.endswith(\"best.pt\"):\n",
    "            bests.append(root)\n",
    "    if not bests:\n",
    "        raise FileNotFoundError(f\"No best.pt found under: {ckpt_glob}\")\n",
    "    return sorted(bests)[-1]\n",
    "\n",
    "best_path = load_best(CKPT)\n",
    "print(\"Loading:\", best_path)\n",
    "\n",
    "ckpt = torch.load(best_path, map_location=\"cpu\")\n",
    "model_cfg = ckpt.get(\"model_cfg\", {})  # if your training saved it\n",
    "model = dc_lite.DCLiteLM(**{\n",
    "    # sane defaults; override by saved config if present:\n",
    "    \"vocab_size\": 256,\n",
    "    \"d_model_tok\": model_cfg.get(\"d_model_tok\", 256),\n",
    "    \"d_model_chunk\": model_cfg.get(\"d_model_chunk\", 384),\n",
    "    \"n_layers_tok\": model_cfg.get(\"n_layers_tok\", 2),\n",
    "    \"n_heads_tok\": model_cfg.get(\"n_heads_tok\", 4),\n",
    "    \"n_layers_chunk\": model_cfg.get(\"n_layers_chunk\", 4),\n",
    "    \"n_heads_chunk\": model_cfg.get(\"n_heads_chunk\", 6),\n",
    "    \"mlp_mult\": model_cfg.get(\"mlp_mult\", 2.0),\n",
    "    \"dropout\": model_cfg.get(\"dropout\", 0.3),\n",
    "    \"target_chunk_len\": model_cfg.get(\"target_chunk_len\", 64),\n",
    "    \"boundary_rate_weight\": model_cfg.get(\"boundary_rate_weight\", 0.03),\n",
    "    \"smooth_tau\": model_cfg.get(\"smooth_tau\", 0.6),\n",
    "})\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.to(DEVICE).eval()\n",
    "\n",
    "# --- EVALUATION: collect true/pred byte histograms ----------------------------\n",
    "true_counts = Counter()\n",
    "pred_counts = Counter()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_stream():\n",
    "    for x, y in tqdm(dl, total=len(dl)):\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        y = y.to(DEVICE, non_blocking=True)\n",
    "        logits, _, _ = model(x, return_aux=True)\n",
    "        pred = torch.argmax(logits, dim=-1)  # [B,T]\n",
    "        # Accumulate histograms over bytes\n",
    "        for t in y.flatten().tolist():\n",
    "            true_counts[int(t)] += 1\n",
    "        for p in pred.flatten().tolist():\n",
    "            pred_counts[int(p)] += 1\n",
    "\n",
    "eval_stream()\n",
    "\n",
    "# --- Utility: discrete divergence (KL and Jensen-Shannon) ---------------------\n",
    "def to_prob(counts):\n",
    "    total = sum(counts.values())\n",
    "    p = np.zeros(256, dtype=np.float64)\n",
    "    if total > 0:\n",
    "        for k, v in counts.items():\n",
    "            p[int(k)] = v / total\n",
    "    return p\n",
    "\n",
    "def kl_div(p, q, eps=1e-12):  # KL(p||q)\n",
    "    p = np.clip(p, eps, 1.0); q = np.clip(q, eps, 1.0)\n",
    "    return float(np.sum(p * np.log(p / q)))\n",
    "\n",
    "def js_div(p, q, eps=1e-12):\n",
    "    m = 0.5*(p+q)\n",
    "    return 0.5*kl_div(p, m, eps) + 0.5*kl_div(q, m, eps)\n",
    "\n",
    "P_true = to_prob(true_counts)\n",
    "P_pred = to_prob(pred_counts)\n",
    "print(\"Global byte-level KL(true||pred):\", kl_div(P_true, P_pred))\n",
    "print(\"Global byte-level JS:\", js_div(P_true, P_pred))\n",
    "\n",
    "# --- Focused metrics: Event Type (1..7) and Direction (-1/+1) -----------------\n",
    "# We try BOTH encodings:\n",
    "#   (A) numeric bytes 1..7, 255/… (for bit_packed), and -1/+1 often show up as 255? varies.\n",
    "#   (B) ASCII digits '1'..'7' -> 49..55 and '-' -> 45 (for utf8_delim).\n",
    "# We'll count both and report whichever mass is non-negligible.\n",
    "\n",
    "def extract_event_type_hist(counts):\n",
    "    # numeric 1..7\n",
    "    evt_numeric = np.array([counts.get(i, 0) for i in range(1,8)], dtype=np.float64)\n",
    "    # ASCII '1'..'7'\n",
    "    evt_ascii = np.array([counts.get(i, 0) for i in range(49,56)], dtype=np.float64)\n",
    "    return evt_numeric, evt_ascii\n",
    "\n",
    "def extract_direction_hist(counts):\n",
    "    # numeric: -1/+1 impossible as bytes; sometimes 1 is used for buy and 255 for signed? unknown in bit_packed.\n",
    "    # ASCII: '-' (45) followed by '1' (49) for -1; and '1'(49) alone for +1 (depending on delim format).\n",
    "    # As a simple proxy, we read counts at ASCII '-' and '1'.\n",
    "    dir_ascii = np.array([counts.get(45, 0), counts.get(49, 0)], dtype=np.float64)  # [-, +]\n",
    "    return dir_ascii\n",
    "\n",
    "def norm_hist(h):\n",
    "    s = h.sum()\n",
    "    return h/s if s>0 else h\n",
    "\n",
    "evt_true_num, evt_true_asc = extract_event_type_hist(true_counts)\n",
    "evt_pred_num, evt_pred_asc = extract_event_type_hist(pred_counts)\n",
    "\n",
    "# Choose the encoding with more mass\n",
    "use_ascii_evt = (evt_true_asc.sum() + evt_pred_asc.sum()) > (evt_true_num.sum() + evt_pred_num.sum())\n",
    "if use_ascii_evt:\n",
    "    Ht_evt, Hp_evt = norm_hist(evt_true_asc), norm_hist(evt_pred_asc)\n",
    "    evt_labels = [str(i) for i in range(1,8)]\n",
    "    title_evt = \"Event Type (ASCII digits '1'..'7')\"\n",
    "else:\n",
    "    Ht_evt, Hp_evt = norm_hist(evt_true_num), norm_hist(evt_pred_num)\n",
    "    evt_labels = [str(i) for i in range(1,8)]\n",
    "    title_evt = \"Event Type (numeric bytes 1..7)\"\n",
    "\n",
    "print(f\"[Event Type] KL(true||pred)={kl_div(Ht_evt, Hp_evt):.4f}, JS={js_div(Ht_evt, Hp_evt):.4f}\")\n",
    "\n",
    "dir_true = norm_hist(extract_direction_hist(true_counts))\n",
    "dir_pred = norm_hist(extract_direction_hist(pred_counts))\n",
    "print(f\"[Direction -/+ (ASCII proxy)] KL(true||pred)={kl_div(dir_true, dir_pred):.4f}, JS={js_div(dir_true, dir_pred):.4f}\")\n",
    "\n",
    "# --- Plots: histograms of EventType and Direction -----------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_bar_comp(labels, p_true, p_pred, title, fname):\n",
    "    x = np.arange(len(labels))\n",
    "    w = 0.38\n",
    "    plt.figure(figsize=(7,4.5))\n",
    "    plt.bar(x-w/2, p_true, width=w, label=\"true\")\n",
    "    plt.bar(x+w/2, p_pred, width=w, label=\"pred\")\n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylabel(\"probability\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "plot_bar_comp(evt_labels, Ht_evt, Hp_evt, title_evt, \"event_type_dist.png\")\n",
    "plot_bar_comp([\"-1\",\" +1 (proxy)\"], dir_true, dir_pred, \"Direction (ASCII proxy)\", \"direction_dist.png\")\n",
    "\n",
    "# --- Optional: PnL from forecast sign and future log-returns ------------------\n",
    "def compute_future_log_return(df, horizon):\n",
    "    \"\"\"\n",
    "    df must contain columns: time, midprice (float)\n",
    "    returns aligned vectors: times[:-h], fret (future log return over horizon)\n",
    "    \"\"\"\n",
    "    p = df[MIDPRICE_PRICE_COL].astype(float).values\n",
    "    fret = np.log(p[horizon:] / p[:-horizon])\n",
    "    t = df[MIDPRICE_TIME_COL].values[:-horizon]\n",
    "    return t, fret\n",
    "\n",
    "def forecast_sign_from_bytes(pred_counts_local=None):\n",
    "    \"\"\"\n",
    "    VERY SIMPLE PROXY:\n",
    "      sign(fcst_t) is approximated by the difference between probabilities of direction '+' vs '-'\n",
    "      measured on the predicted stream around direction tokens (ASCII proxy).\n",
    "    For a stronger estimate you would decode rows and pick the Direction field only.\n",
    "    \"\"\"\n",
    "    if pred_counts_local is None:\n",
    "        pred_counts_local = pred_counts\n",
    "    p_minus = pred_counts_local.get(45, 0)  # '-'\n",
    "    p_plus  = pred_counts_local.get(49, 0)  # '1' (used in '+1')\n",
    "    s = p_plus - p_minus\n",
    "    return 1.0 if s >= 0 else -1.0\n",
    "\n",
    "def pnl_from_fcst_and_fret(fcst_sign, fret):\n",
    "    return fcst_sign * fret\n",
    "\n",
    "if MIDPRICE_CSV and os.path.exists(MIDPRICE_CSV):\n",
    "    df_mid = pd.read_csv(MIDPRICE_CSV)\n",
    "    horizons = [1,5,10,20]\n",
    "    fcst_s = forecast_sign_from_bytes()  # scalar sign; if you can align per-timestep, replace by a vector\n",
    "    for h in horizons:\n",
    "        times, fret = compute_future_log_return(df_mid, h)\n",
    "        pnl = pnl_from_fcst_and_fret(fcst_s, fret)\n",
    "        print(f\"h={h:>2d}  meanPnL={pnl.mean(): .5e}  Sharpe={pnl.mean()/pnl.std(): .3f}  n={len(pnl)}\")\n",
    "else:\n",
    "    print(\"PnL skipped (no midprice file provided). Supply MIDPRICE_CSV to compute PnL.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6TIbZSKbC4vQ",
    "BK5pXbgmUgLU",
    "069SOi4qTp7j",
    "8fSmx2MvTtaT",
    "nwzSAHa4LA8l",
    "7y2dJ_IGnIu_",
    "xCPAe0WSTwll",
    "a_pjIHVkp6ua"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
